{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tc import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tc.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import tc.config as config\n",
    "# from tc.data_import import ImportData\n",
    "from tc.distance_calc import DistanceCalculation\n",
    "from tc.results_page import ResultCalculations ,calculate_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregating Columns\n",
    "\n",
    "categorical_columns = ['Zip','ID','TC']\n",
    "numerical_columns = ['NRx','PDE','Copay',\n",
    "                     'Delivered_Doximity','Delivered_Epocrates','Engaged_Doximity',\n",
    "                     'Engaged_Epocrates','Delivered_Medscape','Engaged_Medscape',\n",
    "                     'Delivered_Nexgen','Engaged_Nexgen','SpeakerEvent']\n",
    "hcp_identifier = 'ID'\n",
    "\n",
    "\n",
    "date_columns='Month'\n",
    "date_format='yyyymm'\n",
    "format = ['long']\n",
    "tc_identifier='TC'\n",
    "event_identifer='SpeakerEvent'\n",
    "test_value='T'\n",
    "control_value='C'\n",
    "segment_var=[]\n",
    "\n",
    "\n",
    "format_mapping = {\n",
    "    \"yyyy-mm\": \"%Y-%m\",\n",
    "    \"yyyymm\": \"%Y%m\",\n",
    "    \"dd-mm-yyyy\": \"%d-%m-%Y\",\n",
    "    \"mm-yyyy\": \"%m-%Y\",\n",
    "    \"yyyy-mm-dd\": \"%Y-%m-%d\",\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pre_start='Jul-2019'\n",
    "pre_end='Dec-2019'  \n",
    "event_date='Jan-2020'\n",
    "post_end='Jun-2020'\n",
    "\n",
    "# For aggregration variable specification\n",
    "#cases = {'Emails': [5],'Calls':[5],'Brand_Sales':[5]}\n",
    "\n",
    "# Define cases specifying positions for each prefix\n",
    "cases = {\n",
    "    'NRx': {6: ['pre']},   \n",
    "    #'Mobile_Alerts': {8: ['pre']}\n",
    "                  # For prefix 'B', select columns from the end\n",
    "           # For prefix 'B', select columns from the beginning\n",
    "}\n",
    "\n",
    "event_vars =['NRx']\n",
    "agg_vars=['NRx']\n",
    "sales_var='NRx'\n",
    "\n",
    "\n",
    "mom_pre_wts=[1,1,1]\n",
    "mom_post_wts=[1,1,1]\n",
    "\n",
    "agg_pre_wts=[1,1,1]\n",
    "agg_post_wts=[1,1,1]\n",
    "\n",
    "mom_pre_match=True\n",
    "mom_post_match=False\n",
    "\n",
    "agg_pre_match=True\n",
    "agg_post_match=False\n",
    "\n",
    "\n",
    "#Defining batch size\n",
    "\n",
    "batch_size = 50\n",
    "result_df = pd.DataFrame(columns=['Test_ID', 'Control_ID', 'Euclidean Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tomlkit import date\n",
    "import tc.config as config\n",
    "import pdb\n",
    "from tc.log_config import setup_logging\n",
    "setup_logging()\n",
    "\n",
    "import logging\n",
    "# Import Module:\n",
    "\n",
    "class ImportData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        data_format: str,\n",
    "        date_format: str,\n",
    "        date_col: str,\n",
    "        categorical_cols: list,\n",
    "        numerical_cols: list,\n",
    "        identifier: str,\n",
    "        week : bool\n",
    "    ):\n",
    "        self.file_path = file_path\n",
    "        self.data_format = data_format\n",
    "        self.date_format = date_format\n",
    "        self.date_col = date_col\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.identifier = identifier\n",
    "        self.week=week\n",
    "\n",
    "    def read_csv(self):\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(f\"File not found at {self.file_path}\")\n",
    "            raise FileNotFoundError(f\"File not found at {self.file_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error occurred while reading {self.file_path} - {str(e)}\")\n",
    "            raise Exception(f\"Unexpected error occurred: {str(e)}\")\n",
    "        else:\n",
    "            if self.data_format == \"long\":\n",
    "                df = self.pivot_long_to_wide(df)\n",
    "\n",
    "            logging.info(f\"File {self.file_path} imported successfully.\")\n",
    "\n",
    "            return df\n",
    "\n",
    "    def date_conversion(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            data = data.sort_values(self.date_col)\n",
    "            if self.date_format in format_mapping:\n",
    "                format_string = format_mapping[self.date_format]\n",
    "                data[self.date_col] = pd.to_datetime(\n",
    "                    data[self.date_col], format=format_string\n",
    "                ).dt.strftime(\"%b%y\")\n",
    "            logging.info(f\"Date conversion was successfully.\")\n",
    "            return data\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Date conversion error: {e}.\")\n",
    "            raise Exception(f\"Error: {e}. Please provide correct date format\")\n",
    "\n",
    "\n",
    "    def pivot_long_to_wide(self, data) -> pd.DataFrame:\n",
    "        # try:\n",
    "        # Pivot the dataframe\n",
    "        data = data.filter(\n",
    "            regex=f\"^({self.numerical_cols}|{self.categorical_cols}|{self.date_col})\",\n",
    "            axis=1\n",
    "        )\n",
    "        data = self.date_conversion(data=data)\n",
    "        unique_date_values = list(data[self.date_col].unique())\n",
    "        data[\"Date1\"] = pd.Categorical(\n",
    "            data[self.date_col], categories=unique_date_values, ordered=True\n",
    "        )\n",
    "        print('Here1')\n",
    "        pivot_df = data.pivot_table(\n",
    "            index=self.identifier, columns=\"Date1\", values=self.numerical_cols # type: ignore\n",
    "        )\n",
    "        print('Here1')\n",
    "\n",
    "        # Flatten the multi-level column index\n",
    "        pivot_df.columns = [\n",
    "            \"_\".join(col).rstrip(\"_\") for col in pivot_df.columns.values\n",
    "        ]\n",
    "        # Reset the index\n",
    "        pivot_df = pd.DataFrame(pivot_df)\n",
    "        pivot_df.reset_index(inplace=True)\n",
    "        merged_data = pd.merge(\n",
    "            pivot_df,\n",
    "            data.drop_duplicates(subset=[self.identifier]),\n",
    "            on=self.identifier,\n",
    "        )\n",
    "        logging.info(f\"Pivoting was successfully.\")\n",
    "        return merged_data\n",
    "        # except Exception as e:\n",
    "        #     logging.error(f\"An error occurred during pivoting: {e}\")\n",
    "        #     print(f\"An error occurred:Please upload long format data,Check data for long-wide conversion\")\n",
    "        #     raise Exception(\"Pivoting error, check data format and structure.\")\n",
    "        #     return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tc.analysis_months_calculation import analyis_initial_months\n",
    "import tc.config as config\n",
    "from tc.table_creation import generate_variables\n",
    "from tc.multipliers import apply_multipliers_table\n",
    "from tc.time_align_var_func import increment_numbers_in_list\n",
    "from tc.agg_logic import calculate_sum_last_n_columns,standardize_columns\n",
    "from scipy.spatial.distance import cdist\n",
    "import pdb\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_start_var='Jul-2019'\n",
    "pre_end_var='Dec-2019'  \n",
    "event_date_var='Jan-2020'\n",
    "post_end_var='Jun-2020'\n",
    "\n",
    "class DistanceCalculation:\n",
    "    def __init__(self,data,num_cols,hcp_identifier,tc_identifier,event_identifier):\n",
    "        self.data = data\n",
    "        self.num_cols=num_cols\n",
    "        self.date_length=None\n",
    "        self.hcp_identifier=hcp_identifier\n",
    "        self.tc_identifier=tc_identifier\n",
    "        self.event_identifier=event_identifier\n",
    "\n",
    "    def time_align(self,test_filter_value,post_cut_off)-> pd.DataFrame:\n",
    "        selected_data = self.data.filter(regex=f'^({self.hcp_identifier}|{self.event_identifier}|{self.tc_identifier})', axis=1)\n",
    "        selected_data=selected_data[selected_data[self.tc_identifier] == test_filter_value]\n",
    "        selected_columns=list(selected_data)\n",
    "        selected_columns.remove(self.hcp_identifier)\n",
    "        selected_columns.remove(self.tc_identifier)\n",
    "        num_months=len(selected_columns)\n",
    "        self.date_length=num_months\n",
    "        mapping_dict={'Months':selected_columns,'Index': list(range(1,num_months+1))}\n",
    "        selected_data.drop([self.tc_identifier],inplace=True,axis=1)\n",
    "        pre_post_calc=analyis_initial_months(event_date=event_date_var,pre_start=pre_start_var,post_end= post_end_var,pre_end=pre_end_var)\n",
    "        self.mapping_month=mapping_dict['Index'][mapping_dict['Months'].index(f\"{self.event_identifier}_{pre_post_calc['event_date']}\")]\n",
    "        first_exposure=selected_data.loc[:,  f\"{self.event_identifier}_{pre_post_calc['event_date']}\":].gt(0).idxmax(axis=1).apply(lambda x:str(x))\n",
    "        # Create a mapping dictionary using zip\n",
    "        mapping_dict = dict(zip(mapping_dict['Months'], mapping_dict['Index']))\n",
    "        # Replace values in the \"Post_Start\" column based on the mapping dictionary\n",
    "        time_align=pd.DataFrame({self.hcp_identifier: selected_data[self.hcp_identifier], 'Post_Start': first_exposure})\n",
    "        time_align['Post_Start'] = time_align['Post_Start'].replace(mapping_dict)\n",
    "        time_align['Post_End']=time_align['Post_Start']+pre_post_calc['total_months_post']-1\n",
    "        time_align['Pre_End']=time_align['Post_Start']-1\n",
    "        time_align['Pre_Start']=time_align['Post_Start']-pre_post_calc['total_months_pre']\n",
    "        #condition for boundary cases\n",
    "        time_align['Post_End'] = time_align['Post_End'].apply(lambda x: x if x <= num_months else num_months)\n",
    "        \n",
    "        # 3 months post period cut off condition\n",
    "        post_start_check = sorted(time_align['Post_Start'].unique())\n",
    "        post_end_check = sorted(time_align['Post_End'].unique())\n",
    "        result = []\n",
    "        for i in range(len(post_start_check)):\n",
    "            if i < len(post_end_check) and abs(post_end_check[i] - post_start_check[i]) <= post_cut_off:\n",
    "                result.append(post_start_check[i])\n",
    "        if len(result)>=0:\n",
    "             time_align=time_align[~time_align['Post_Start'].isin(result)]\n",
    "        self.time_aligned_data=time_align\n",
    "        return time_align\n",
    "    \n",
    "\n",
    "    def column_conversion(self):\n",
    "        date_length=self.date_length\n",
    "        categ_values = [f\"{col}_{i}\" for col in self.num_cols for i in range(1, date_length + 1)]\n",
    "\n",
    "        # Define a pattern using regular expression\n",
    "        pattern = re.compile(r'Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec',re.IGNORECASE)\n",
    "\n",
    "        # Filter columns based on the pattern\n",
    "        selected_columns = list(filter(lambda col: pattern.search(col), self.data.columns))\n",
    "\n",
    "        for vars in segment_var:\n",
    "            if vars in selected_columns:\n",
    "                selected_columns.remove(vars)\n",
    "                \n",
    "        remaining_columns = [col for col in self.data.columns if col not in selected_columns]\n",
    "\n",
    "        # Rename columns\n",
    "        # Define the desired order of columns\n",
    "        desired_colums=remaining_columns+selected_columns\n",
    "        # Reorder the columns\n",
    "        data = self.data.reindex(columns=desired_colums)\n",
    "\n",
    "        new_column_names = remaining_columns + categ_values\n",
    "        data.rename(columns=dict(zip(data.columns, new_column_names)), inplace=True)\n",
    "        self.data=data\n",
    "        self.agg_data=data.copy()\n",
    "        return data\n",
    "\n",
    "    def data_standardize(self):\n",
    "    # Assuming 'data' is your DataFrame with both numerical and non-numerical columns\n",
    "    # Select only the numerical columns\n",
    "        numerical_columns = self.data.select_dtypes(include=['float64', 'int64']).columns\n",
    "        numerical_columns=numerical_columns.delete(numerical_columns.get_loc(self.hcp_identifier))\n",
    "    # Create a StandardScaler object\n",
    "        #scaler = StandardScaler()\n",
    "        # Fit the scaler on the selected numerical columns and transform the data\n",
    "        #self.data[numerical_columns] = scaler.fit_transform(self.data[numerical_columns])\n",
    "        #self.data[numerical_columns] = preprocessing.scale(self.data[numerical_columns])\n",
    "         # Calculate mean and standard deviation with ddof=1\n",
    "        column_means = self.data[numerical_columns].mean()\n",
    "        column_stds = self.data[numerical_columns].std(ddof=1)\n",
    "\n",
    "        # Standardize the data using mean and standard deviation with ddof=1\n",
    "        standardized_data = (self.data[numerical_columns] - column_means) / column_stds\n",
    "\n",
    "        # Update the original DataFrame with standardized values\n",
    "        self.data[numerical_columns] = standardized_data\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def distance_matching(self,control_matches,agg_matching,segment_matching,filter_test_val,filter_control_val,segment_var,batch_size):\n",
    "        if segment_matching==True:\n",
    "            self.data['segment_var'] = self.data[segment_var].astype(str).apply(lambda x: \"_\".join(x), axis=1)\n",
    "            self.agg_data['segment_var'] = self.agg_data[segment_var].astype(str).apply(lambda x: \"_\".join(x), axis=1)\n",
    "        else:\n",
    "            self.data['segment_var']=\"National\"\n",
    "            self.agg_data['segment_var']=\"National\"\n",
    "        uni_combo=sorted(self.time_aligned_data['Pre_Start'].unique())\n",
    "        pre_post_calc=analyis_initial_months(event_date=event_date_var,pre_start=pre_start_var,post_end= post_end_var,pre_end=pre_end_var)\n",
    "        mom_variables_to_match,table = generate_variables(self.time_aligned_data,event_vars,pre_wts=mom_pre_wts,post_wts=mom_post_wts,pre_match=mom_pre_match,post_match=mom_post_match)\n",
    "        mom_variables_to_match.append(self.hcp_identifier)\n",
    "        mom_match=increment_numbers_in_list(mom_variables_to_match,times=self.time_aligned_data['Pre_Start'].nunique()-1)\n",
    "        # Create a mapping of Pre_Start to index\n",
    "        pre_start_index_mapping = {value: index for index, value in enumerate(uni_combo)}\n",
    "        uni_combo1 = [(pre_start_var, segment, pre_start_index_mapping[pre_start_var])\n",
    "                for pre_start_var, segment in product(uni_combo, self.data['segment_var'].unique())]\n",
    "        \n",
    "        test_df=self.data[self.data[self.tc_identifier] == filter_test_val]\n",
    "        control_df=self.data[self.data[self.tc_identifier] == filter_control_val]\n",
    "        test_df=pd.merge(test_df,self.time_aligned_data,on=self.hcp_identifier)\n",
    "        result_df = pd.DataFrame(columns=['Test_ID', 'Control_ID', 'Euclidean Distance']) \n",
    "\n",
    "\n",
    "        for pre_start_var,segment,index in uni_combo1:\n",
    "            test_df_segmented = test_df[(test_df['Pre_Start'] == pre_start_var) & (test_df['segment_var'] == segment)]\n",
    "            for start_idx in range(0, test_df_segmented.shape[0], batch_size):\n",
    "                end_idx = min(start_idx + batch_size, test_df_segmented.shape[0])\n",
    "                test_df_batch = test_df_segmented.iloc[start_idx:end_idx]\n",
    "                test_df1=test_df_batch[(test_df_batch['Pre_Start'] == pre_start_var) & (test_df_batch['segment_var'] == segment)]\n",
    "                mom_vec_filter=[x for x in mom_match[index] if x in self.data.columns]\n",
    "                test_df1=pd.DataFrame(test_df1[mom_vec_filter])\n",
    "                control_df1=control_df[control_df['segment_var'] == segment]\n",
    "                control_df1=pd.DataFrame(control_df1[mom_vec_filter])\n",
    "                apply_multipliers_table(self.time_aligned_data,test_df1, table,pre_post_calc)\n",
    "                apply_multipliers_table(self.time_aligned_data,control_df1, table,pre_post_calc)\n",
    "                if agg_matching==True:\n",
    "                    agg_variables_to_match,table1 = generate_variables(self.time_aligned_data,agg_vars,pre_wts=agg_pre_wts,post_wts=agg_post_wts,pre_match=agg_pre_match,post_match=agg_post_match)\n",
    "                    agg_variables_to_match.append(self.hcp_identifier)\n",
    "                    agg_match=increment_numbers_in_list(agg_variables_to_match,times=self.time_aligned_data['Pre_Start'].nunique()-1)\n",
    "                    test_df_agg=pd.merge(self.agg_data,self.time_aligned_data,on=self.hcp_identifier)\n",
    "                    agg_vec_filter=[x for x in agg_match[index] if x in self.data.columns]\n",
    "                    test_df_agg=test_df_agg[test_df_agg['Pre_Start'] == pre_start_var]\n",
    "                    test_df_agg1=calculate_sum_last_n_columns(test_df_agg,pre_post_calc,cases,agg_vec_filter,self.mapping_month)\n",
    "                    control_df_agg1=calculate_sum_last_n_columns(self.agg_data[self.agg_data[self.tc_identifier] == filter_control_val],pre_post_calc,cases,agg_vec_filter,self.mapping_month)\n",
    "                    agg_data=pd.concat([test_df_agg1,control_df_agg1])\n",
    "                    subset_columns_agg = [col for col in agg_data.columns if col.startswith(\"Sum_\")]\n",
    "                    standardize_columns(agg_data,subset_columns_agg)\n",
    "                    agg_data=agg_data[agg_data['segment_var'] == segment]\n",
    "                    apply_multipliers_table(self.time_aligned_data,agg_data,table1,pre_post_calc)\n",
    "                    test_df1=pd.merge(test_df1,agg_data,on=self.hcp_identifier)\n",
    "                    control_df1=pd.merge(control_df1,agg_data,on=self.hcp_identifier)\n",
    "                    #test_df1 = test_df1.iloc[start_idx:end_idx]\n",
    "                    new_list=mom_vec_filter\n",
    "                    new_list+=subset_columns_agg\n",
    "                else:\n",
    "                    new_list=mom_vec_filter\n",
    "                new_list = [x for x in new_list if x != self.hcp_identifier]\n",
    "                distances = cdist(test_df1[new_list].values, control_df1[new_list].values)\n",
    "                batch_top_n_indices = np.argsort(distances, axis=1)[:, :control_matches]\n",
    "                batch_top_n_control_ids = control_df1[self.hcp_identifier].values[batch_top_n_indices]\n",
    "                batch_top_n_distances = np.take_along_axis(distances, batch_top_n_indices, axis=1)\n",
    "                if control_df1.shape[0]!=0:\n",
    "                    col_result_df = pd.DataFrame({'Test_ID': np.repeat(test_df1[self.hcp_identifier], control_matches),\n",
    "                                  'Control_ID': batch_top_n_control_ids.flatten(),\n",
    "                                  'Euclidean Distance': batch_top_n_distances.flatten()})\n",
    "                    result_df = pd.concat([result_df, col_result_df], ignore_index=True)\n",
    "            \n",
    "                \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID,Zip,NRx,TRx,MktNRx,MktTRx,PDE,Copay,Delivered_Doximity,\n",
    "# Delivered_Epocrates,Engaged_Doximity,Engaged_Epocrates,Delivered_Medscape,\n",
    "# Engaged_Medscape,Delivered_Nexgen,Engaged_Nexgen,SpeakerEvent,Month,Cluster,\n",
    "# TV_GRPs,Website_Sessions,Display_Impressions,Display_Clicks,Print_GRPs,OTT_IMPRS,\n",
    "# TV2_GRPs,Website2_Sessions,Display2_Impressions,Display2_Clicks,Print2_GRPs,OTT2_IMPRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Month'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the datatype of the column\n",
    "# data['Month'].dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"MMX_Data_PDE_Reduced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert it to str\n",
    "# data['Month'] = data['Month'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"MMX_Data_PDE_Reduced_1.csv\")\n",
    "data['TC'] = np.where(np.random.rand(len(data)) < 0.15, 'T', 'C')\n",
    "# save the dataset\n",
    "# data.to_csv(\"MMX_Data_PDE_Reduced.csv\", index=False)\n",
    "data.to_csv(\"MMX_Data_PDE_Reduced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12',\n",
       "       '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    C\n",
       "Name: TC, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"MMX_Data_PDE_Reduced.csv\")\n",
    "data['TC'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Zip</th>\n",
       "      <th>NRx</th>\n",
       "      <th>TRx</th>\n",
       "      <th>MktNRx</th>\n",
       "      <th>MktTRx</th>\n",
       "      <th>PDE</th>\n",
       "      <th>Copay</th>\n",
       "      <th>Delivered_Doximity</th>\n",
       "      <th>Delivered_Epocrates</th>\n",
       "      <th>...</th>\n",
       "      <th>Display_Impressions</th>\n",
       "      <th>Display_Clicks</th>\n",
       "      <th>Print_GRPs</th>\n",
       "      <th>OTT_IMPRS</th>\n",
       "      <th>TV2_GRPs</th>\n",
       "      <th>Website2_Sessions</th>\n",
       "      <th>Display2_Impressions</th>\n",
       "      <th>Display2_Clicks</th>\n",
       "      <th>Print2_GRPs</th>\n",
       "      <th>OTT2_IMPRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14934</td>\n",
       "      <td>48307</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>377</td>\n",
       "      <td>80</td>\n",
       "      <td>973</td>\n",
       "      <td>708</td>\n",
       "      <td>438</td>\n",
       "      <td>979</td>\n",
       "      <td>622</td>\n",
       "      <td>347</td>\n",
       "      <td>729</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14934</td>\n",
       "      <td>48307</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>427</td>\n",
       "      <td>122</td>\n",
       "      <td>583</td>\n",
       "      <td>66</td>\n",
       "      <td>805</td>\n",
       "      <td>305</td>\n",
       "      <td>906</td>\n",
       "      <td>683</td>\n",
       "      <td>846</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14934</td>\n",
       "      <td>48307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>588</td>\n",
       "      <td>382</td>\n",
       "      <td>550</td>\n",
       "      <td>188</td>\n",
       "      <td>495</td>\n",
       "      <td>263</td>\n",
       "      <td>212</td>\n",
       "      <td>131</td>\n",
       "      <td>471</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14934</td>\n",
       "      <td>48307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>609</td>\n",
       "      <td>534</td>\n",
       "      <td>149</td>\n",
       "      <td>732</td>\n",
       "      <td>465</td>\n",
       "      <td>402</td>\n",
       "      <td>344</td>\n",
       "      <td>119</td>\n",
       "      <td>100</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14934</td>\n",
       "      <td>48307</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>683</td>\n",
       "      <td>717</td>\n",
       "      <td>177</td>\n",
       "      <td>723</td>\n",
       "      <td>307</td>\n",
       "      <td>761</td>\n",
       "      <td>230</td>\n",
       "      <td>383</td>\n",
       "      <td>123</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID    Zip  NRx  TRx  MktNRx  MktTRx   PDE  Copay  Delivered_Doximity  \\\n",
       "0  14934  48307    4    0       1       1   7.7      4                   0   \n",
       "1  14934  48307    2    0       0       0   1.9      5                   1   \n",
       "2  14934  48307    1    1       2       2  10.6      4                   0   \n",
       "3  14934  48307    0    0       0       0   8.6      2                   3   \n",
       "4  14934  48307    3    0       1       1   4.7      3                   0   \n",
       "\n",
       "   Delivered_Epocrates  ...  Display_Impressions  Display_Clicks  Print_GRPs  \\\n",
       "0                    0  ...                  377              80         973   \n",
       "1                    1  ...                  427             122         583   \n",
       "2                    0  ...                  588             382         550   \n",
       "3                    0  ...                  609             534         149   \n",
       "4                    0  ...                  683             717         177   \n",
       "\n",
       "   OTT_IMPRS  TV2_GRPs  Website2_Sessions  Display2_Impressions  \\\n",
       "0        708       438                979                   622   \n",
       "1         66       805                305                   906   \n",
       "2        188       495                263                   212   \n",
       "3        732       465                402                   344   \n",
       "4        723       307                761                   230   \n",
       "\n",
       "  Display2_Clicks  Print2_GRPs  OTT2_IMPRS  \n",
       "0             347          729         408  \n",
       "1             683          846         410  \n",
       "2             131          471         498  \n",
       "3             119          100         917  \n",
       "4             383          123         893  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(\"MMX_Data_PDE_Reduced_1.csv\")\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['Zip','ID','TC']\n",
    "numerical_columns = ['NRx','PDE','Copay',\n",
    "                     'Delivered_Doximity','Delivered_Epocrates','Engaged_Doximity',\n",
    "                     'Engaged_Epocrates','Delivered_Medscape','Engaged_Medscape',\n",
    "                     'Delivered_Nexgen','Engaged_Nexgen','SpeakerEvent']\n",
    "hcp_identifier = 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Page module\n",
    "\n",
    "dataset_import = ImportData(\n",
    "    file_path=\"MMX_Data_PDE_Reduced.csv\",\n",
    "    data_format=\"long\",\n",
    "    date_format=\"yyyy-mm\",\n",
    "    date_col='Month',\n",
    "    categorical_cols=categorical_columns,\n",
    "    numerical_cols=numerical_columns,\n",
    "    identifier=hcp_identifier,\n",
    "    week=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here1\n",
      "Here1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25620\\3061558036.py:77: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot_df = data.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "read_data = dataset_import.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Copay_Jul19</th>\n",
       "      <th>Copay_Aug19</th>\n",
       "      <th>Copay_Sep19</th>\n",
       "      <th>Copay_Oct19</th>\n",
       "      <th>Copay_Nov19</th>\n",
       "      <th>Copay_Dec19</th>\n",
       "      <th>Copay_Jan20</th>\n",
       "      <th>Copay_Feb20</th>\n",
       "      <th>Copay_Mar20</th>\n",
       "      <th>...</th>\n",
       "      <th>TV_GRPs</th>\n",
       "      <th>Display_Impressions</th>\n",
       "      <th>Display_Clicks</th>\n",
       "      <th>Print_GRPs</th>\n",
       "      <th>TV2_GRPs</th>\n",
       "      <th>Display2_Impressions</th>\n",
       "      <th>Display2_Clicks</th>\n",
       "      <th>Print2_GRPs</th>\n",
       "      <th>TC</th>\n",
       "      <th>Date1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14934</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>279</td>\n",
       "      <td>377</td>\n",
       "      <td>80</td>\n",
       "      <td>973</td>\n",
       "      <td>438</td>\n",
       "      <td>622</td>\n",
       "      <td>347</td>\n",
       "      <td>729</td>\n",
       "      <td>C</td>\n",
       "      <td>Jul19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14935</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>918</td>\n",
       "      <td>506</td>\n",
       "      <td>723</td>\n",
       "      <td>886</td>\n",
       "      <td>777</td>\n",
       "      <td>943</td>\n",
       "      <td>621</td>\n",
       "      <td>495</td>\n",
       "      <td>C</td>\n",
       "      <td>Jul19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14936</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>94</td>\n",
       "      <td>998</td>\n",
       "      <td>354</td>\n",
       "      <td>306</td>\n",
       "      <td>650</td>\n",
       "      <td>219</td>\n",
       "      <td>559</td>\n",
       "      <td>C</td>\n",
       "      <td>Jul19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14937</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>425</td>\n",
       "      <td>416</td>\n",
       "      <td>978</td>\n",
       "      <td>565</td>\n",
       "      <td>268</td>\n",
       "      <td>375</td>\n",
       "      <td>265</td>\n",
       "      <td>242</td>\n",
       "      <td>T</td>\n",
       "      <td>Jul19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14938</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>921</td>\n",
       "      <td>447</td>\n",
       "      <td>242</td>\n",
       "      <td>672</td>\n",
       "      <td>713</td>\n",
       "      <td>276</td>\n",
       "      <td>734</td>\n",
       "      <td>740</td>\n",
       "      <td>C</td>\n",
       "      <td>Jul19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Copay_Jul19  Copay_Aug19  Copay_Sep19  Copay_Oct19  Copay_Nov19  \\\n",
       "0  14934          4.0          5.0          4.0          2.0          3.0   \n",
       "1  14935          5.0          2.0          2.0          1.0          4.0   \n",
       "2  14936          3.0          1.0          1.0          3.0          5.0   \n",
       "3  14937          4.0          3.0          1.0          2.0          2.0   \n",
       "4  14938          2.0          3.0          3.0          1.0          2.0   \n",
       "\n",
       "   Copay_Dec19  Copay_Jan20  Copay_Feb20  Copay_Mar20  ...  TV_GRPs  \\\n",
       "0          4.0          5.0          4.0          3.0  ...      279   \n",
       "1          4.0          1.0          2.0          4.0  ...      918   \n",
       "2          4.0          2.0          5.0          3.0  ...      157   \n",
       "3          2.0          4.0          5.0          3.0  ...      425   \n",
       "4          4.0          2.0          4.0          1.0  ...      921   \n",
       "\n",
       "   Display_Impressions  Display_Clicks  Print_GRPs  TV2_GRPs  \\\n",
       "0                  377              80         973       438   \n",
       "1                  506             723         886       777   \n",
       "2                   94             998         354       306   \n",
       "3                  416             978         565       268   \n",
       "4                  447             242         672       713   \n",
       "\n",
       "   Display2_Impressions  Display2_Clicks  Print2_GRPs  TC  Date1  \n",
       "0                   622              347          729   C  Jul19  \n",
       "1                   943              621          495   C  Jul19  \n",
       "2                   650              219          559   C  Jul19  \n",
       "3                   375              265          242   T  Jul19  \n",
       "4                   276              734          740   C  Jul19  \n",
       "\n",
       "[5 rows x 173 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\n",
      "Copay_Jul19\n",
      "Copay_Aug19\n",
      "Copay_Sep19\n",
      "Copay_Oct19\n",
      "Copay_Nov19\n",
      "Copay_Dec19\n",
      "Copay_Jan20\n",
      "Copay_Feb20\n",
      "Copay_Mar20\n",
      "Copay_Apr20\n",
      "Copay_May20\n",
      "Copay_Jun20\n",
      "Delivered_Doximity_Jul19\n",
      "Delivered_Doximity_Aug19\n",
      "Delivered_Doximity_Sep19\n",
      "Delivered_Doximity_Oct19\n",
      "Delivered_Doximity_Nov19\n",
      "Delivered_Doximity_Dec19\n",
      "Delivered_Doximity_Jan20\n",
      "Delivered_Doximity_Feb20\n",
      "Delivered_Doximity_Mar20\n",
      "Delivered_Doximity_Apr20\n",
      "Delivered_Doximity_May20\n",
      "Delivered_Doximity_Jun20\n",
      "Delivered_Epocrates_Jul19\n",
      "Delivered_Epocrates_Aug19\n",
      "Delivered_Epocrates_Sep19\n",
      "Delivered_Epocrates_Oct19\n",
      "Delivered_Epocrates_Nov19\n",
      "Delivered_Epocrates_Dec19\n",
      "Delivered_Epocrates_Jan20\n",
      "Delivered_Epocrates_Feb20\n",
      "Delivered_Epocrates_Mar20\n",
      "Delivered_Epocrates_Apr20\n",
      "Delivered_Epocrates_May20\n",
      "Delivered_Epocrates_Jun20\n",
      "Delivered_Medscape_Jul19\n",
      "Delivered_Medscape_Aug19\n",
      "Delivered_Medscape_Sep19\n",
      "Delivered_Medscape_Oct19\n",
      "Delivered_Medscape_Nov19\n",
      "Delivered_Medscape_Dec19\n",
      "Delivered_Medscape_Jan20\n",
      "Delivered_Medscape_Feb20\n",
      "Delivered_Medscape_Mar20\n",
      "Delivered_Medscape_Apr20\n",
      "Delivered_Medscape_May20\n",
      "Delivered_Medscape_Jun20\n",
      "Delivered_Nexgen_Jul19\n",
      "Delivered_Nexgen_Aug19\n",
      "Delivered_Nexgen_Sep19\n",
      "Delivered_Nexgen_Oct19\n",
      "Delivered_Nexgen_Nov19\n",
      "Delivered_Nexgen_Dec19\n",
      "Delivered_Nexgen_Jan20\n",
      "Delivered_Nexgen_Feb20\n",
      "Delivered_Nexgen_Mar20\n",
      "Delivered_Nexgen_Apr20\n",
      "Delivered_Nexgen_May20\n",
      "Delivered_Nexgen_Jun20\n",
      "Engaged_Doximity_Jul19\n",
      "Engaged_Doximity_Aug19\n",
      "Engaged_Doximity_Sep19\n",
      "Engaged_Doximity_Oct19\n",
      "Engaged_Doximity_Nov19\n",
      "Engaged_Doximity_Dec19\n",
      "Engaged_Doximity_Jan20\n",
      "Engaged_Doximity_Feb20\n",
      "Engaged_Doximity_Mar20\n",
      "Engaged_Doximity_Apr20\n",
      "Engaged_Doximity_May20\n",
      "Engaged_Doximity_Jun20\n",
      "Engaged_Epocrates_Jul19\n",
      "Engaged_Epocrates_Aug19\n",
      "Engaged_Epocrates_Sep19\n",
      "Engaged_Epocrates_Oct19\n",
      "Engaged_Epocrates_Nov19\n",
      "Engaged_Epocrates_Dec19\n",
      "Engaged_Epocrates_Jan20\n",
      "Engaged_Epocrates_Feb20\n",
      "Engaged_Epocrates_Mar20\n",
      "Engaged_Epocrates_Apr20\n",
      "Engaged_Epocrates_May20\n",
      "Engaged_Epocrates_Jun20\n",
      "Engaged_Medscape_Jul19\n",
      "Engaged_Medscape_Aug19\n",
      "Engaged_Medscape_Sep19\n",
      "Engaged_Medscape_Oct19\n",
      "Engaged_Medscape_Nov19\n",
      "Engaged_Medscape_Dec19\n",
      "Engaged_Medscape_Jan20\n",
      "Engaged_Medscape_Feb20\n",
      "Engaged_Medscape_Mar20\n",
      "Engaged_Medscape_Apr20\n",
      "Engaged_Medscape_May20\n",
      "Engaged_Medscape_Jun20\n",
      "Engaged_Nexgen_Jul19\n",
      "Engaged_Nexgen_Aug19\n",
      "Engaged_Nexgen_Sep19\n",
      "Engaged_Nexgen_Oct19\n",
      "Engaged_Nexgen_Nov19\n",
      "Engaged_Nexgen_Dec19\n",
      "Engaged_Nexgen_Jan20\n",
      "Engaged_Nexgen_Feb20\n",
      "Engaged_Nexgen_Mar20\n",
      "Engaged_Nexgen_Apr20\n",
      "Engaged_Nexgen_May20\n",
      "Engaged_Nexgen_Jun20\n",
      "NRx_Jul19\n",
      "NRx_Aug19\n",
      "NRx_Sep19\n",
      "NRx_Oct19\n",
      "NRx_Nov19\n",
      "NRx_Dec19\n",
      "NRx_Jan20\n",
      "NRx_Feb20\n",
      "NRx_Mar20\n",
      "NRx_Apr20\n",
      "NRx_May20\n",
      "NRx_Jun20\n",
      "PDE_Jul19\n",
      "PDE_Aug19\n",
      "PDE_Sep19\n",
      "PDE_Oct19\n",
      "PDE_Nov19\n",
      "PDE_Dec19\n",
      "PDE_Jan20\n",
      "PDE_Feb20\n",
      "PDE_Mar20\n",
      "PDE_Apr20\n",
      "PDE_May20\n",
      "PDE_Jun20\n",
      "SpeakerEvent_Jul19\n",
      "SpeakerEvent_Aug19\n",
      "SpeakerEvent_Sep19\n",
      "SpeakerEvent_Oct19\n",
      "SpeakerEvent_Nov19\n",
      "SpeakerEvent_Dec19\n",
      "SpeakerEvent_Jan20\n",
      "SpeakerEvent_Feb20\n",
      "SpeakerEvent_Mar20\n",
      "SpeakerEvent_Apr20\n",
      "SpeakerEvent_May20\n",
      "SpeakerEvent_Jun20\n",
      "Zip\n",
      "NRx\n",
      "TRx\n",
      "MktNRx\n",
      "MktTRx\n",
      "PDE\n",
      "Copay\n",
      "Delivered_Doximity\n",
      "Delivered_Epocrates\n",
      "Engaged_Doximity\n",
      "Engaged_Epocrates\n",
      "Delivered_Medscape\n",
      "Engaged_Medscape\n",
      "Delivered_Nexgen\n",
      "Engaged_Nexgen\n",
      "SpeakerEvent\n",
      "Month\n",
      "Cluster\n",
      "TV_GRPs\n",
      "Display_Impressions\n",
      "Display_Clicks\n",
      "Print_GRPs\n",
      "TV2_GRPs\n",
      "Display2_Impressions\n",
      "Display2_Clicks\n",
      "Print2_GRPs\n",
      "TC\n",
      "Date1\n"
     ]
    }
   ],
   "source": [
    "for col in read_data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25620\\519834542.py:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  time_align['Post_Start'] = time_align['Post_Start'].replace(mapping_dict)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_25620\\519834542.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data['segment_var']=\"National\"\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'pre_start_var' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m original_data_conversion \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(data_conversion)\n\u001b[0;32m     20\u001b[0m dist_calc\u001b[38;5;241m.\u001b[39mdata_standardize()\n\u001b[1;32m---> 22\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mdist_calc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_matching\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43mcontrol_matches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43magg_matching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43msegment_matching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43mfilter_test_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43mfilter_control_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43msegment_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# # #Result Page Module\u001b[39;00m\n\u001b[0;32m     37\u001b[0m result_calc\u001b[38;5;241m=\u001b[39mResultCalculations(\n\u001b[0;32m     38\u001b[0m     matching_data\u001b[38;5;241m=\u001b[39mresult,\n\u001b[0;32m     39\u001b[0m     time_aligned_data\u001b[38;5;241m=\u001b[39mtime_aligned_data,\n\u001b[0;32m     40\u001b[0m     data_conversion\u001b[38;5;241m=\u001b[39moriginal_data_conversion,\n\u001b[0;32m     41\u001b[0m     hcp_identifier\u001b[38;5;241m=\u001b[39mhcp_identifier\n\u001b[0;32m     42\u001b[0m )\n",
      "Cell \u001b[1;32mIn[145], line 112\u001b[0m, in \u001b[0;36mDistanceCalculation.distance_matching\u001b[1;34m(self, control_matches, agg_matching, segment_matching, filter_test_val, filter_control_val, segment_var, batch_size)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_var\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNational\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m uni_combo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_aligned_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPre_Start\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m--> 112\u001b[0m pre_post_calc\u001b[38;5;241m=\u001b[39manalyis_initial_months(event_date\u001b[38;5;241m=\u001b[39mevent_date_var,pre_start\u001b[38;5;241m=\u001b[39m\u001b[43mpre_start_var\u001b[49m,post_end\u001b[38;5;241m=\u001b[39m post_end_var,pre_end\u001b[38;5;241m=\u001b[39mpre_end_var)\n\u001b[0;32m    113\u001b[0m mom_variables_to_match,table \u001b[38;5;241m=\u001b[39m generate_variables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_aligned_data,event_vars,pre_wts\u001b[38;5;241m=\u001b[39mmom_pre_wts,post_wts\u001b[38;5;241m=\u001b[39mmom_post_wts,pre_match\u001b[38;5;241m=\u001b[39mmom_pre_match,post_match\u001b[38;5;241m=\u001b[39mmom_post_match)\n\u001b[0;32m    114\u001b[0m mom_variables_to_match\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhcp_identifier)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'pre_start_var' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "event_identifers = ['SpeakerEvent']\n",
    "\n",
    "\n",
    "res = {}\n",
    "avg_table_dict = {}\n",
    "\n",
    "for event_identifier in event_identifers:\n",
    "    dist_calc=DistanceCalculation(\n",
    "    data=read_data,\n",
    "    num_cols=numerical_columns,\n",
    "    hcp_identifier=hcp_identifier,\n",
    "    tc_identifier=tc_identifier,\n",
    "    event_identifier=event_identifier)\n",
    "\n",
    "    time_aligned_data=dist_calc.time_align(test_filter_value=test_value,post_cut_off=0)\n",
    "\n",
    "    data_conversion=dist_calc.column_conversion()\n",
    "\n",
    "    original_data_conversion = copy.deepcopy(data_conversion)\n",
    "    dist_calc.data_standardize()\n",
    "\n",
    "    result=dist_calc.distance_matching(\n",
    "    control_matches=1,\n",
    "    agg_matching=True,\n",
    "    segment_matching=False,\n",
    "    filter_test_val=test_value,\n",
    "    filter_control_val=control_value,\n",
    "    segment_var=segment_var,\n",
    "    batch_size=batch_size\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # #Result Page Module\n",
    "\n",
    "    result_calc=ResultCalculations(\n",
    "        matching_data=result,\n",
    "        time_aligned_data=time_aligned_data,\n",
    "        data_conversion=original_data_conversion,\n",
    "        hcp_identifier=hcp_identifier\n",
    "    )\n",
    "    result_calc.data_merge()\n",
    "    avgs_data=result_calc.avgs_calculation()\n",
    "    graph_data_test,graph_data_control=result_calc.graph_data_generation()\n",
    "\n",
    "    avgs_table,lift=calculate_lift(avgs_data,sales_var)\n",
    "\n",
    "    # add lift of event_identifier to result dictionary\n",
    "    res[event_identifier]=lift\n",
    "    avg_table_dict[event_identifier]=avgs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Speaker_Program': 27.326325078945242, 'Mobile_Alerts': 30.51659942957498}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Speaker_Program': {'pre_test_avg': 45.44365,\n",
       "  'post_test_avg': 54.5375,\n",
       "  'pre_control_avg': 36.3694,\n",
       "  'post_control_avg': 34.285},\n",
       " 'Mobile_Alerts': {'pre_test_avg': 44.8287,\n",
       "  'post_test_avg': 54.51625,\n",
       "  'pre_control_avg': 36.382675,\n",
       "  'post_control_avg': 33.67875}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
